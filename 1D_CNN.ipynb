{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1892b2-fb67-4f73-9585-0aac3c4fb74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is 1d CNN with CONV1D+ temporal pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3eff9-87e0-430e-8043-2ddfe67de3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input (2 × 18)\n",
    "# → Conv1D (kernel=3)\n",
    "# → BatchNorm\n",
    "# → ReLU\n",
    "# → Conv1D (kernel=3)\n",
    "# → BatchNorm\n",
    "# → ReLU\n",
    "# → Adaptive Pooling\n",
    "# → Shared Representation\n",
    "#    ├── Class Head (Softmax)\n",
    "#    └── Anomaly Head (Sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77def9d1-53e1-4fae-9120-e75f109dec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "854dfacd-3603-47e9-91c6-9f8527a49e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    \"Tot Fwd Pkts\",\n",
    "    \"Tot Bwd Pkts\",\n",
    "    \"Flow Pkts/s\",\n",
    "    \"Flow Byts/s\",\n",
    "    \"Flow Duration\",\n",
    "    \"Flow IAT Mean\",\n",
    "    \"Flow IAT Std\",\n",
    "    \"Fwd IAT Mean\",\n",
    "    \"Bwd IAT Mean\",\n",
    "    \"Pkt Len Mean\",\n",
    "    \"Pkt Len Std\",\n",
    "    \"Pkt Len Var\",\n",
    "    \"SYN Flag Cnt\",\n",
    "    \"ACK Flag Cnt\",\n",
    "    \"RST Flag Cnt\",\n",
    "    \"FIN Flag Cnt\",\n",
    "    \"Down/Up Ratio\",\n",
    "    \"Init Fwd Win Byts\"\n",
    "]\n",
    "\n",
    "Label_MAP = {\n",
    "    \"Normal\": 0,\n",
    "    \"DDOS\":1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c10564c6-a479-490a-a26b-7bb732a51634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset class (Preprocessing logic)\n",
    "window_size=10\n",
    "class SDNIDSDataset(Dataset):\n",
    "    def __init__(self, csv_path, window_size=10, time_steps=2):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.window_size = window_size\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        self._clean_dataframe()\n",
    "        self.samples, self.Labels = self._build_samples()\n",
    "\n",
    "    def _clean_dataframe(self):\n",
    "        self.df.columns = [c.strip() for c in self.df.columns]\n",
    "\n",
    "        for feat in FEATURES:\n",
    "            if feat not in self.df.columns:\n",
    "                self.df[feat] = 0.0\n",
    "\n",
    "        self.df = self.df.dropna(subset=[\"Label\"])\n",
    "        self.df[\"Label\"] = self.df[\"Label\"].astype(int)   #.map(Label_MAP)\n",
    "        self.df = self.df.dropna(subset=[\"Label\"])\n",
    "        self.df[\"Label\"] = self.df[\"Label\"].astype(int)\n",
    "\n",
    "        self.df[FEATURES] = self.df[FEATURES].replace([np.inf, -np.inf], 0)\n",
    "        self.df[FEATURES] = self.df[FEATURES].fillna(0)\n",
    "        \n",
    "        self.df = self.df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def _aggregate_window(self, window_df):\n",
    "        return window_df[FEATURES].mean().values.astype(np.float32)\n",
    "\n",
    "#code i adapter after having all the window as an attack\n",
    "    def _build_samples(self):\n",
    "        windows = []\n",
    "        window_Labels = []\n",
    "    \n",
    "        # Sliding window with stride = 1\n",
    "        for i in range(0, len(self.df) - self.window_size + 1):\n",
    "            window_df = self.df.iloc[i:i + self.window_size]\n",
    "    \n",
    "            feat_vec = self._aggregate_window(window_df)\n",
    "    \n",
    "            # # Use majority vote instead of OR-label\n",
    "            # Label = 1 if window_df[\"Label\"].mean() > 0.5 else 0\n",
    "            # OR threshold\n",
    "            threshold = 2\n",
    "            Label = 1 if window_df[\"Label\"].sum() >= threshold else 0\n",
    "    \n",
    "            if i < 5:  # debug\n",
    "                print(\"Window Labels:\", window_df[\"Label\"].tolist(), \"->\", Label)\n",
    "    \n",
    "            windows.append(feat_vec)\n",
    "            window_Labels.append(Label)\n",
    "    \n",
    "        samples = []\n",
    "        Labels = []\n",
    "    \n",
    "        for i in range(self.time_steps - 1, len(windows)):\n",
    "            stacked = np.stack(\n",
    "                windows[i - self.time_steps + 1:i + 1],\n",
    "                axis=0\n",
    "            )\n",
    "            samples.append(stacked)\n",
    "            Labels.append(window_Labels[i])\n",
    "    \n",
    "        return (\n",
    "            torch.from_numpy(np.array(samples, dtype=np.float32)),\n",
    "            torch.from_numpy(np.array(Labels, dtype=np.int64))\n",
    "        )\n",
    "   \n",
    "# #original code wwe wrote at the start\n",
    "#     def _build_samples(self):\n",
    "#         windows = []\n",
    "#         window_Labels = []\n",
    "    \n",
    "#         # IMPORTANT: sort by time\n",
    "#         if \"Timestamp\" in self.df.columns:\n",
    "#             self.df = self.df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    \n",
    "#         # Sliding window with stride = 1\n",
    "#         for i in range(0, len(self.df) - self.window_size + 1):\n",
    "#             window_df = self.df.iloc[i:i + self.window_size]\n",
    "    \n",
    "#             feat_vec = self._aggregate_window(window_df)\n",
    "#             Label = window_df[\"Label\"].max()  # IDS OR-Labeling\n",
    "    \n",
    "#             if i < 5:  # debug\n",
    "#                 print(\"Window Labels:\", window_df[\"Label\"].tolist(), \"->\", Label)\n",
    "    \n",
    "#             windows.append(feat_vec)\n",
    "#             window_Labels.append(Label)\n",
    "    \n",
    "#         samples = []\n",
    "#         Labels = []\n",
    "    \n",
    "#         for i in range(self.time_steps - 1, len(windows)):\n",
    "#             stacked = np.stack(\n",
    "#                 windows[i - self.time_steps + 1:i + 1],\n",
    "#                 axis=0\n",
    "#             )\n",
    "#             samples.append(stacked)\n",
    "#             Labels.append(window_Labels[i])\n",
    "    \n",
    "#         return (\n",
    "#             torch.from_numpy(np.array(samples, dtype=np.float32)),\n",
    "#             torch.from_numpy(np.array(Labels, dtype=np.int64))\n",
    "#         )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.Labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b5f91344-bc83-49d7-993c-141c977b8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disp_pakt', 'rate_pkt_in', 'disp_interval', 'mean_byte', 'gfe', 'g_usip', 'rfip', 'avg_flow_dst', 'Label', 'switch_id', 'time', 'mean_pkt', 'disp_byte', 'avg_durat', 'gsp']\n",
      "   disp_pakt  rate_pkt_in  disp_interval  mean_byte  gfe  g_usip  rfip  \\\n",
      "0   0.000000  2151.000000   4.810000e+17        0.0    0       0   0.0   \n",
      "1   0.000000   493.600000   1.800000e+17        0.0   17       8   0.0   \n",
      "2  78.342760   518.333333   2.060000e+17    18417.0    0       0   1.0   \n",
      "3   1.549193   464.000000   3.090000e+17       98.0    0       0   1.0   \n",
      "4   1.549193   578.333333   1.610000e+17       98.0    0       0   1.0   \n",
      "\n",
      "   avg_flow_dst  Label  switch_id                  time   mean_pkt  \\\n",
      "0             3      0          2  11/16/2022, 12:17:10   0.000000   \n",
      "1           282      1          6  11/16/2022, 09:55:34   0.000000   \n",
      "2            84      0          2  02/06/2023, 21:25:06  12.380952   \n",
      "3             6      0          3  02/21/2023, 08:24:06   1.000000   \n",
      "4             6      0          3  02/21/2023, 09:10:31   1.000000   \n",
      "\n",
      "       disp_byte     avg_durat  gsp  \n",
      "0       0.000000  4.600000e+17    0  \n",
      "1       0.000000  5.660000e+17   17  \n",
      "2  118496.063622  5.000000e+17    0  \n",
      "3     151.820947  7.110000e+17    0  \n",
      "4     151.820947  5.750000e+17    0  \n",
      "(28323, 15)\n",
      "Label\n",
      "0    18457\n",
      "1     9866\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test the csv file here\n",
    "\n",
    "df = pd.read_csv(\"ICMP_Flood_Attack.csv\")\n",
    "print(df.columns.tolist())\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "64064e21-cd56-4feb-ae88-c28658ecd99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "df[\"Label\"] = df[\"Label\"].map(Label_MAP)\n",
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "514d7b2f-939c-49fc-ba07-534d544105ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Labels: [0, 0, 0, 0, 1, 1, 0, 0, 0, 0] -> 1\n",
      "Window Labels: [0, 0, 0, 1, 1, 0, 0, 0, 0, 0] -> 1\n",
      "Window Labels: [0, 0, 1, 1, 0, 0, 0, 0, 0, 0] -> 1\n",
      "Window Labels: [0, 1, 1, 0, 0, 0, 0, 0, 0, 0] -> 1\n",
      "Window Labels: [1, 1, 0, 0, 0, 0, 0, 0, 0, 0] -> 1\n",
      "Label\n",
      "0    18457\n",
      "1     9866\n",
      "Name: count, dtype: int64\n",
      "Total samples: 28313\n",
      "Unique Labels: tensor([0, 1])\n",
      "Counts: tensor([ 2528, 25785])\n",
      "Sample shape: torch.Size([2, 18])\n"
     ]
    }
   ],
   "source": [
    "#Creating dataset here\n",
    "dataset = SDNIDSDataset(\n",
    "    csv_path=\"ICMP_Flood_Attack.csv\",\n",
    "    # window_size=10,   # ~1 second worth of flows\n",
    "    time_steps=2\n",
    ")\n",
    "df = pd.read_csv(\"ICMP_Flood_Attack.csv\")\n",
    "print(df[\"Label\"].value_counts())\n",
    "print(\"Total samples:\", len(dataset))\n",
    "\n",
    "Labels = dataset.Labels\n",
    "print(\"Unique Labels:\", torch.unique(Labels))\n",
    "print(\"Counts:\", torch.bincount(Labels))\n",
    "print(\"Sample shape:\", dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8ddf01b1-d57d-42f0-889d-d25d22bc8ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([ 2528, 25785]))\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "#testing if dataset is loaded properly and Labelled as well\n",
    "print(np.unique(dataset.Labels.numpy(), return_counts=True))\n",
    "print(df[\"Label\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "654f1185-c398-49e4-8eec-3de44b90219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Label distribution: (array([0, 1]), array([ 2528, 25785]))\n"
     ]
    }
   ],
   "source": [
    "Labels = dataset.Labels.numpy()\n",
    "indices = np.arange(len(dataset))\n",
    "\n",
    "print(\"Overall Label distribution:\", np.unique(Labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45d665-2dc5-4ec3-9baf-6d056a2f65df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "07f03454-d3f1-4555-8592-4be4d4314284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN MODAL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8f8f0141-0320-45df-a28f-41f785cb924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SDN_CNN(nn.Module):\n",
    "    def __init__(self, num_features=18, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv blocks\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=num_features,\n",
    "            out_channels=32,\n",
    "            kernel_size=2,\n",
    "            padding=0\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Global pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Heads\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "        self.anomaly_head = nn.Linear(64, 1)  # optional, future use\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, F)\n",
    "        \"\"\"\n",
    "        # Rearrange for Conv1D\n",
    "        x = x.permute(0, 2, 1)  # (B, F, T)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        x = self.global_pool(x).squeeze(-1)  # (B, 64)\n",
    "\n",
    "        class_logits = self.classifier(x)\n",
    "        anomaly_score = torch.sigmoid(self.anomaly_head(x))\n",
    "\n",
    "        return class_logits, anomaly_score\n",
    "\n",
    "#testing        \n",
    "#print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "42f25812-98c2-455f-857f-1e8ef4d8d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 2])\n",
      "Anomaly score: 0.49741655588150024\n"
     ]
    }
   ],
   "source": [
    "#Sanity check of CNN modal up\n",
    "model = SDN_CNN(num_features=18, num_classes=2)\n",
    "model.eval()  # IMPORTANT\n",
    "\n",
    "x, y = dataset[0]\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, anomaly = model(x)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Anomaly score:\", anomaly.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1d0346b0-189e-4af3-9ab9-4dab5ffeafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset   = Subset(dataset, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cecf073d-a3ad-4df8-a955-bf9d3ff5a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitiing the dataset\n",
    "Labels = dataset.Labels.numpy()\n",
    "indices = np.arange(len(dataset))\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    stratify=dataset.Labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "95136d7e-0247-423a-9f90-4b1e04e02ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels: (array([0, 1]), array([ 2022, 20628]))\n",
      "Val Labels: (array([0, 1]), array([ 506, 5157]))\n"
     ]
    }
   ],
   "source": [
    "#verifiy Spliting\n",
    "train_Labels = Labels[train_idx]\n",
    "val_Labels   = Labels[val_idx]\n",
    "\n",
    "print(\"Train Labels:\", np.unique(train_Labels, return_counts=True))\n",
    "print(\"Val Labels:\", np.unique(val_Labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "81d11463-8754-41e2-87d5-22f9e81d9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training modal now\n",
    "#DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b3757e5d-9c2f-464c-b66b-f412ec82af93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#check if cuda is being used of not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "12ba760c-987b-4e00-9e91-38ca33ecb671",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[256]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m weights = class_counts.sum() / (\u001b[32m2\u001b[39m * class_counts)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# class_weights = torch.tensor(\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     class_weights,\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#     dtype=torch.float32\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ).to(device)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m class_weights = \u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m().clone().to(device)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClass counts:\u001b[39m\u001b[33m\"\u001b[39m, class_counts)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClass weights:\u001b[39m\u001b[33m\"\u001b[39m, class_weights)\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "#CLASS WEIGHTS\n",
    "class_counts = np.bincount(train_Labels)\n",
    "\n",
    "weights = class_counts.sum() / (2 * class_counts)\n",
    "# class_weights = torch.tensor(\n",
    "#     class_weights,\n",
    "#     dtype=torch.float32\n",
    "# ).to(device)\n",
    "class_weights = weights.detach().clone().to(device)\n",
    "print(\"Class counts:\", class_counts)\n",
    "print(\"Class weights:\", class_weights)\n",
    "print(\"Type:\", type(class_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "6681ace8-ffa7-476b-b9db-3e59528d7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model, Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "model = SDN_CNN(\n",
    "    num_features=18,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1f3a7afd-a024-4f54-9c51-a55e5ab46c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function fo one as to keep gpu safe and easier to see whats happening\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for X, y in loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "\n",
    "    return avg_loss, acc, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "06ff8cad-156d-4fb6-9bd9-50f96d5dd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation loop\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits, _ = model(X)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "\n",
    "    return acc, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9a809dfe-726a-48d7-ac3e-a20625cb6f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window label distribution: {np.int64(0): np.int64(14694), np.int64(1): np.int64(13620)}\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "window_labels = []\n",
    "for i in range(len(df) - window_size + 1):\n",
    "    win = df.iloc[i:i+window_size]\n",
    "    label = 1 if win[\"Label\"].sum() >= threshold else 0\n",
    "    window_labels.append(label)\n",
    "\n",
    "unique, counts = np.unique(window_labels, return_counts=True)\n",
    "print(\"Window label distribution:\", dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7a7b3ea6-6711-4ec4-8470-728bca75712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.6905 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 02 | Train Loss: 0.6904 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 03 | Train Loss: 0.6906 | Train Acc: 0.9096 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 04 | Train Loss: 0.6903 | Train Acc: 0.9096 | Train F1: 0.9526 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 05 | Train Loss: 0.6911 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 06 | Train Loss: 0.6907 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 07 | Train Loss: 0.6905 | Train Acc: 0.9096 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 08 | Train Loss: 0.6906 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 09 | Train Loss: 0.6908 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 10 | Train Loss: 0.6900 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 11 | Train Loss: 0.6905 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 12 | Train Loss: 0.6907 | Train Acc: 0.9096 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 13 | Train Loss: 0.6898 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 14 | Train Loss: 0.6909 | Train Acc: 0.9096 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 15 | Train Loss: 0.6901 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 16 | Train Loss: 0.6893 | Train Acc: 0.9098 | Train F1: 0.9528 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 17 | Train Loss: 0.6895 | Train Acc: 0.9096 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 18 | Train Loss: 0.6902 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 19 | Train Loss: 0.6900 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 20 | Train Loss: 0.6904 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 21 | Train Loss: 0.6896 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 22 | Train Loss: 0.6907 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 23 | Train Loss: 0.6904 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 24 | Train Loss: 0.6900 | Train Acc: 0.9098 | Train F1: 0.9528 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 25 | Train Loss: 0.6900 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 26 | Train Loss: 0.6906 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 27 | Train Loss: 0.6909 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 28 | Train Loss: 0.6895 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 29 | Train Loss: 0.6914 | Train Acc: 0.9097 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n",
      "Epoch 30 | Train Loss: 0.6907 | Train Acc: 0.9096 | Train F1: 0.9527 | Val Acc: 0.9149 | Val F1: 0.9556\n"
     ]
    }
   ],
   "source": [
    "#actual final training\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    val_acc, val_f1 = evaluate(\n",
    "        model,\n",
    "        val_loader,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Train F1: {train_f1:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f} | \"\n",
    "        f\"Val F1: {val_f1:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "912f8b01-babd-43a4-8899-4fdd3212310a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> 2\n"
     ]
    }
   ],
   "source": [
    "out = model(x)\n",
    "print(type(out), len(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "214eb693-c010-4401-9590-83742a3cf504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred counts: (array([0]), array([11326]))\n",
      "True counts: (array([0, 1]), array([ 482, 5181]))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits,_ = model(x)\n",
    "        probs  = torch.sigmoid(logits)\n",
    "        preds  = (probs >= 0.5).long()\n",
    "\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(y.cpu())\n",
    "\n",
    "all_preds  = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "print(\"Pred counts:\", np.unique(all_preds, return_counts=True))\n",
    "print(\"True counts:\", np.unique(all_labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b4c57c9d-3cbe-41e2-9742-c6072654f154",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found empty input array (e.g., `y_true` or `y_pred`) while a minimum of 1 sample is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[250]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:558\u001b[39m, in \u001b[36mconfusion_matrix\u001b[39m\u001b[34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[39m\n\u001b[32m    549\u001b[39m     y_type, y_true, y_pred, sample_weight = _check_targets(\n\u001b[32m    550\u001b[39m         y_true, y_pred, sample_weight\n\u001b[32m    551\u001b[39m     )\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    553\u001b[39m     \u001b[38;5;66;03m# This is needed to handle the special case where y_true, y_pred and\u001b[39;00m\n\u001b[32m    554\u001b[39m     \u001b[38;5;66;03m# sample_weight are all empty.\u001b[39;00m\n\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# In this case we don't pass sample_weight to _check_targets that would\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;66;03m# check that sample_weight is not empty and we don't reuse the returned\u001b[39;00m\n\u001b[32m    557\u001b[39m     \u001b[38;5;66;03m# sample_weight\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     y_type, y_true, y_pred, _ = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:113\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m array \u001b[38;5;129;01min\u001b[39;00m [y_true, y_pred]:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _num_samples(array) < \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    114\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound empty input array (e.g., `y_true` or `y_pred`) while a minimum \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    115\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mof 1 sample is required.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m         )\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    118\u001b[39m     sample_weight = _check_sample_weight(\n\u001b[32m    119\u001b[39m         sample_weight, y_true, force_float_dtype=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    120\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found empty input array (e.g., `y_true` or `y_pred`) while a minimum of 1 sample is required."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce517e-2245-4014-a9ef-27a70650614d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
